{
 "metadata": {
  "name": "",
  "signature": "sha256:b9956b72ce67799519b5c5b316515f546d95b666d9727eb40d047ef59f82e700"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "print 'pandas version: ',pd.__version__\n",
      "print 'numpy version:',np.__version__\n",
      "print 'sklearn version:',sk.__version__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "pandas version:  0.13.1\n",
        "numpy version: 1.8.1\n",
        "sklearn version: 0.14.1\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys,os\n",
      "cwd=os.getcwd()\n",
      "path=cwd.split('/')\n",
      "home_dir='/'.join(path[:-2])\n",
      "print home_dir\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dir=home_dir+'/data/weather'\n",
      "!ls $data_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ALL.head.csv\t  ghcnd-stations_buffered.txt  SAMPLE_TMAX.csv\r\n",
        "data-source.txt   ghcnd-stations.txt\t       SAMPLE_TMAX.csv.old.gz\r\n",
        "ghcnd-readme.txt  ghcnd-version.txt\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls *.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "coding.py\t  mr_job_01.py\t\t reduce-year-temp.py\r\n",
        "ECatch.py\t  mr_weather.py\t\t Stations_Statistics.py\r\n",
        "map-year-temp.py  mr_word_freq_count.py  Statistics.py\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ECatch.py\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefor to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    print type(func)\n",
      "    f_name=func.__name__\n",
      "    @wraps(func)\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ECatch.py\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefor to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    print type(func)\n",
      "    f_name=func.__name__\n",
      "    @wraps(func)\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class C:\n",
      "    def increment_counter(self,a,b,n):\n",
      "        print 'increment counter(',a,b,n,')'\n",
      "    @ECatch\n",
      "    def Z(self,list):\n",
      "        print list\n",
      "        print sum(list)\n",
      "        return sum(list)\n",
      "CC=C()\n",
      "print CC.Z([1,'a',2])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'function'>\n",
        "increment counter( C total in Z 1 )\n",
        "[1, 'a', 2]\n",
        "increment counter( C errors in Z 1 )\n",
        "None\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Error:cannot perform reduce with flexible typeTraceback (most recent call last):\n",
        "  File \"<ipython-input-6-41c5e6a4d868>\", line 20, in inner\n",
        "    return func(self,*args,**kwargs)\n",
        "  File \"<ipython-input-7-2acc89746305>\", line 7, in Z\n",
        "    print sum(list)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py\", line 1709, in sum\n",
        "    out=out, keepdims=keepdims)\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/numpy/core/_methods.py\", line 25, in _sum\n",
        "    out=out, keepdims=keepdims)\n",
        "TypeError: cannot perform reduce with flexible type\n",
        "Arguments were ([1, 'a', 2],), {}\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CC.__class__.__name__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "'C'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "type(f),f.im_class.__name__,f.im_func.__name__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'builtin_function_or_method' object has no attribute 'im_class'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-9-56751d3d2e50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'im_class'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile Stations_Statistics.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "collect the statistics for each station.\n",
      "\"\"\"\n",
      "import re,pickle,base64,zlib\n",
      "from sys import stderr\n",
      "import sys\n",
      "\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages') # a hack because anaconda made mrjob unreachable\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import *\n",
      "\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "\n",
      "\"\"\"this decorator is intended for decorating a function, not a\n",
      "generator.  Therefor to use it in the context of mrjob, the generator\n",
      "should call a function that handles a single input records, and that\n",
      "function should be decorated.\n",
      "\n",
      "The reason is that if a generator throws an exception it exits and\n",
      "cannot process any more records.\n",
      "\n",
      "\"\"\"\n",
      "def ECatch(func):\n",
      "    f_name=func.__name__\n",
      "    @wraps(func)\n",
      "    def inner(self,*args,**kwargs):\n",
      "        try:\n",
      "            self.increment_counter(self.__class__.__name__,'total in '+f_name,1)\n",
      "            return func(self,*args,**kwargs)\n",
      "        except Exception as e:\n",
      "            self.increment_counter(self.__class__.__name__,'errors in '+f_name,1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            traceback.print_exc(file=stderr)\n",
      "            stderr.write('Arguments were %s, %s\\n'%(args,kwargs))\n",
      "            pass\n",
      "    return inner        \n",
      "\n",
      "\"\"\"\n",
      "Functions for encoding and decoding arbitrary object into ascii \n",
      "so that they can be passed through the hadoop streaming interface.\n",
      "\"\"\"\n",
      "\n",
      "def loads(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def dumps(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "class MRWeather(MRJob):\n",
      "\n",
      "    @ECatch\n",
      "    def map_one(self,line):\n",
      "        return line.split(',')\n",
      "    \n",
      "    def mapper(self, _, line):\n",
      "        elements=self.map_one(line)\n",
      "        yield(elements[0],elements[1:])\n",
      "            \n",
      "    def check_integrity(self,meas,year,length):\n",
      "        year=int(year)\n",
      "        if year<1000 or year > 2014: return False\n",
      "        if meas=='': return False\n",
      "        if length != 367: return False\n",
      "        return True\n",
      "    \n",
      "    @ECatch\n",
      "    def reduce_one(self,S,vector):\n",
      "        meas=vector[0]\n",
      "        year=vector[1]\n",
      "        length=len(vector)\n",
      "        number_defined=sum([e!='' for e in vector[2:]])\n",
      "        assert self.check_integrity(meas,year,length)==True\n",
      "        S[(meas,int(year))]=number_defined\n",
      "        \n",
      "    def reducer(self, station, vectors):\n",
      "        S={}\n",
      "        for vector in vectors:\n",
      "            self.reduce_one(S,vector)\n",
      "        yield(station,dumps(S))\n",
      "                              \n",
      "if __name__ == '__main__':\n",
      "    MRWeather.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting Stations_Statistics.py\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running job inline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -1 $data_dir/ALL.head.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ASN00054128,DAPR,1969,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py --help"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Usage: Stations_Statistics.py [options] [input files]\r\n",
        "\r\n",
        "Options:\r\n",
        "  --help                show this message and exit\r\n",
        "  --help-emr            show EMR-related options\r\n",
        "  --help-hadoop         show Hadoop-related options\r\n",
        "  --help-runner         show runner-related options\r\n",
        "\r\n",
        "  Running specific parts of the job:\r\n",
        "    --mapper            run a mapper\r\n",
        "    --combiner          run a combiner\r\n",
        "    --reducer           run a reducer\r\n",
        "    --step-num=STEP_NUM\r\n",
        "                        which step to execute (default is 0)\r\n",
        "    --steps             print the mappers, combiners, and reducers that this\r\n",
        "                        job defines\r\n",
        "\r\n",
        "  Protocols:\r\n",
        "    --strict-protocols  If something violates an input/output protocol then\r\n",
        "                        raise an exception\r\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py hdfs:/weather/weather.csv > StationStatistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n",
        "Traceback (most recent call last):\r\n",
        "  File \"Stations_Statistics.py\", line 88, in <module>\r\n",
        "    MRWeather.run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 494, in run\r\n",
        "    mr_job.execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 512, in execute\r\n",
        "    super(MRJob, self).execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 147, in execute\r\n",
        "    self.run_job()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 208, in run_job\r\n",
        "    runner.run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/runner.py\", line 458, in run\r\n",
        "    self._run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/sim.py\", line 173, in _run\r\n",
        "    _error_on_bad_paths(self.fs, self._input_paths)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/sim.py\", line 558, in _error_on_bad_paths\r\n",
        "    \"None found in %s\" % paths)\r\n",
        "ValueError: At least one valid path is required. None found in ['hdfs:/weather/weather.csv']\r\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_flow_id=\"j-LTOJMJ14G840\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py -r emr --emr-job-flow-id $job_flow_id hdfs:/weather/weather.csv > StationStatistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/Stations_Statistics.ubuntu.20140525.202830.662477\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140525.202830.662477/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-LTOJMJ14G840\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 60.8s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 91.2s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 121.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 151.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 182.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 212.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 243.0s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 273.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 303.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 334.1s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 364.4s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 394.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 425.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 455.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 486.0s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 516.4s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 546.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 577.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 607.6s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 638.1s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 668.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 698.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 729.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job on job flow j-LTOJMJ14G840 failed with status WAITING: Waiting after step failed\r\n",
        "Logs are in s3://yoav.hadoop/log/j-LTOJMJ14G840/\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Scanning S3 logs for probable cause of failure\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Attempting to terminate job...\r\n",
        "Traceback (most recent call last):\r\n",
        "  File \"Stations_Statistics.py\", line 88, in <module>\r\n",
        "    MRWeather.run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 494, in run\r\n",
        "    mr_job.execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 512, in execute\r\n",
        "    super(MRJob, self).execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 147, in execute\r\n",
        "    self.run_job()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 213, in run_job\r\n",
        "    self.stdout.flush()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/runner.py\", line 614, in __exit__\r\n",
        "    self.cleanup()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1010, in cleanup\r\n",
        "    super(EMRJobRunner, self).cleanup(mode=mode)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/runner.py\", line 560, in cleanup\r\n",
        "    self._cleanup_job()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1084, in _cleanup_job\r\n",
        "    self._opts['ec2_key_pair_file'])\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/ssh.py\", line 200, in ssh_terminate_single_job\r\n",
        "    ssh_bin, address, ec2_key_pair_file, ['hadoop', 'job', '-list']))\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/ssh.py\", line 82, in ssh_run\r\n",
        "    p = Popen(args, stdout=PIPE, stderr=PIPE, stdin=PIPE)\r\n",
        "  File \"/home/ubuntu/anaconda/lib/python2.7/subprocess.py\", line 709, in __init__\r\n",
        "    errread, errwrite)\r\n",
        "  File \"/home/ubuntu/anaconda/lib/python2.7/subprocess.py\", line 1326, in _execute_child\r\n",
        "    raise child_exception\r\n",
        "TypeError: execv() arg 2 must contain only strings\r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## managing AWS Credentials ##\n",
      "The full details on how to get the credentials set up is given here: https://docs.google.com/document/d/1xDUy4ZI2yr1eCCRQ4ynWHsctbEb7ySHrSynBu0bxupU/edit?usp=sharing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!echo $EC2_VAULT"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "Creds_file=os.environ['EC2_VAULT']+'/Creds.pkl'\n",
      "Creds= pickle.load(open(Creds_file,'rb'))\n",
      "print Creds.keys()\n",
      "print Creds['mrjob'].keys()\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "print ID,key_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Checking for an available job flow###\n",
      "Before submitting your job for execution you need to find out which job flows are active and waiting. THe following cell will do that for you. If there is a waiting cluster, it will put it's ID into the variable `job_flow_id`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_flow_id=find_waiting_flow(key_id,secret_key)\n",
      "job_flow_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Setting up the mrjob configuration file ###\n",
      "The last step before submitting the job is to insert the credentials into the file `/home/ubuntu/UCSD_BigData/utils/mrjob.conf.template` and put the result in the default location for the mrjob configuration file which is: `/home/ubuntu/.mrjob.conf`\n",
      "this is done by the following line. If the return value is `True` you are good to go. If it is `False` something went wrong and you should get an error message.\n",
      "\n",
      "The template file should work as is. However, feel free to change and add fields to this configuration file to make it your own.\n",
      "\n",
      "This step needs to be done just one time. Redone only when starting a new EC2 instance or if the credentials changed.\n",
      "It is better to do it in an interactive shell, rather than in a notebook. Here it is done in the notebook for demonstration purpose."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd /home/ubuntu/UCSD_BigData/utils/\n",
      "#!python Make.mrjob.conf.py\n",
      "%cd ../notebooks/weather.mapreduce/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/utils\n",
        "/home/ubuntu/UCSD_BigData/notebooks/weather.mapreduce\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assuming the steps up to here were all successful, you should be ready to launch your mrjob job. There is no need to change anything\n",
      "in the following line."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python Stations_Statistics.py -r emr --emr-job-flow-id  $job_flow_id  hdfs:/weather/weather.csv > Statistics.pkl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/Stations_Statistics.ubuntu.20140525.204117.610147\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140525.204117.610147/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-LTOJMJ14G840\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.4s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 60.8s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 91.1s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 121.5s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 151.9s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 182.3s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 212.7s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 243.1s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 273.5s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 303.9s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 334.2s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 364.6s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 395.0s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 425.5s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 456.0s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 486.2s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 516.4s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 546.9s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 577.3s ago, status RUNNING: Running step (Stations_Statistics.ubuntu.20140525.204117.610147: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job completed.\r\n",
        "Running time was 535.0s (not counting time spent waiting for the EC2 instances)\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Fetching counters from S3...\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters may not have been uploaded to S3 yet. Try again in 5 minutes with: mrjob fetch-logs --counters j-LTOJMJ14G840\r\n",
        "Counters from step 1:\r\n",
        "  (no counters found)\r\n",
        "Streaming final output from s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140525.204117.610147/output/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing tmp directory /tmp/Stations_Statistics.ubuntu.20140525.204117.610147\r\n",
        "Removing all files in s3://yoav.hadoop/scratch/Stations_Statistics.ubuntu.20140525.204117.610147/\r\n"
       ]
      }
     ],
     "prompt_number": 12
    }
   ],
   "metadata": {}
  }
 ]
}